{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ê²½ë¡œì§€ì •ê³¼ csv ì½ëŠ” íŒŒì¼ë§Œ ë³€ê²½í•´ì£¼ë©´ ë¼\n",
        "\n",
        "ëˆ„ì  ì €ì¥ ê°€ëŠ¥\n",
        "\n",
        "200ê°œì”© ì•Œì•„ì„œ ëŠì–´ì„œ í¬ë¡¤ë§í•´ì¤Œ\n",
        "\n",
        "í¬ë¡¤ë§ ì„±ê³µí•œ ê°€ê²Œ ìˆ˜\n",
        "\n",
        "í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ê°€ê²Œ ìˆ˜\n",
        "\n",
        "ì´ í¬ë¡¤ë§í•œ ë¦¬ë·° ê°œìˆ˜ ë§ˆì§€ë§‰ì— í¬í•¨ì‹œí‚´\n",
        "\n",
        "\n",
        "ë³€ê²½í•´ì•¼í•  ë¶€ë¶„ì„ ì»¨íŠ¸ë¡¤ f ë¡œ ê²€ìƒ‰í•˜ë©´ ë­˜ ë³€ê²½í•´ì•¼í• ì§€ ì•Œ ìˆ˜ ì‡ìŒ\n",
        "\n",
        "ëª¨ë¥´ê² ìœ¼ë©´ ë¬¼ì–´ë³´ì…”ìœ "
      ],
      "metadata": {
        "id": "8Wu3Iwz4wDAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service as ChromeService\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "import datetime\n",
        "import os\n",
        "import re\n",
        "\n",
        "crawl = 400 ## ë³€ê²½í•´ì•¼í•  ë¶€ë¶„\n",
        "\n",
        "# ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì •\n",
        "options = Options()\n",
        "options.add_argument(\"window-size=1920x1080\")\n",
        "service = ChromeService(executable_path=ChromeDriverManager().install())\n",
        "driver = webdriver.Chrome(service=service, options=options)\n",
        "\n",
        "# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "df_filtered = (\n",
        "    pd.read_csv('ì„œìš¸ì‹œ íœ´ê²ŒìŒì‹ì  ì¸í—ˆê°€ ì •ë³´-2.csv')\n",
        "    .query('ì—…íƒœêµ¬ë¶„ëª… in [\"ë‹¤ë°©\", \"ì»¤í”¼ìˆ\", \"ë–¡ì¹´í˜\"] and ìƒì„¸ì˜ì—…ìƒíƒœëª… == \"ì˜ì—…\"')\n",
        "    [['ì‚¬ì—…ì¥ëª…', 'ë„ë¡œëª…ì£¼ì†Œ']]\n",
        "    .fillna({'ë„ë¡œëª…ì£¼ì†Œ': ''})\n",
        ")\n",
        "\n",
        "df_filtered['ë„ë¡œëª…ì£¼ì†Œ'] = (\n",
        "    df_filtered['ë„ë¡œëª…ì£¼ì†Œ']\n",
        "    .apply(lambda x: ' '.join(x.split(',')[0].split()[:4]))\n",
        ")\n",
        "\n",
        "records = df_filtered.to_dict(orient='records')\n",
        "now = datetime.datetime.now()\n",
        "\n",
        "# ëˆ„ì  ì €ì¥ì„ ìœ„í•œ CSV ê²½ë¡œ ë° ì»¬ëŸ¼ ì •ì˜\n",
        "csv_path = os.path.expanduser('~/Desktop/naver_review.csv')\n",
        "if os.path.exists(csv_path):\n",
        "    df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
        "\n",
        "    if not df.empty:\n",
        "        last_row = df.iloc[-1]  # ë§ˆì§€ë§‰ í–‰\n",
        "        total_review = last_row['ì´ ë¦¬ë·° ê°œìˆ˜']\n",
        "        already_crawled =  last_row['ìµœì¢… í¬ë¡¤ë§ ê°œìˆ˜']\n",
        "        print(\"ìµœì¢… í¬ë¡¤ë§ ê°œìˆ˜:\", already_crawled)\n",
        "    else:\n",
        "        print(\"CSV íŒŒì¼ì€ ìˆì§€ë§Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "else:\n",
        "    already_crawled = 0 ## ë³€ê²½í•´ì•¼í•  ë¶€ë¶„\n",
        "    total_review = 1\n",
        "    print(\"CSV íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "columns = ['ê³µê³µë°ì´í„° ê°€ê²Œëª…',\n",
        "           'í¬ë¡¤ë§ ê°€ê²Œëª…',\n",
        "           'ì¹´í…Œê³ ë¦¬',\n",
        "           'ì‹œê°„',\n",
        "           'í‰ì ',\n",
        "           'ì¶œì²˜',\n",
        "           'content',\n",
        "           'date',\n",
        "           'revisit',\n",
        "           'ë°©ë¬¸ëª©ì ',\n",
        "           'ë™ë°˜ì',\n",
        "           'íƒœê·¸',\n",
        "           'ì„ íƒ ìˆ˜',\n",
        "           'í¸ì˜ì‹œì„¤',\n",
        "           'ìµœì¢… í¬ë¡¤ë§ ê°œìˆ˜',\n",
        "           'ì´ ë¦¬ë·° ê°œìˆ˜']\n",
        "review_data = []\n",
        "success = 0\n",
        "fail = 0\n",
        "review_cnt = 0\n",
        "crawl_end = already_crawled+crawl\n",
        "if crawl_end > 5324: #ë³€ê²½í•´ì•¼í•  ë¶€ë¶„\n",
        "    crawl_end = 5324\n",
        "\n",
        "# í¬ë¡¤ë§ ì‹œì‘\n",
        "for row in records[already_crawled:crawl_end]:\n",
        "    restaurant = row['ì‚¬ì—…ì¥ëª…']\n",
        "    address = row['ë„ë¡œëª…ì£¼ì†Œ']\n",
        "    driver.get(\"https://m.map.naver.com/#search\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    try:\n",
        "        search_input = driver.find_element(By.XPATH, '//input[contains(@placeholder, \"ê²€ìƒ‰\")]')\n",
        "        search_input.click()\n",
        "        search_input.send_keys(restaurant)\n",
        "        search_input.send_keys(Keys.ENTER)\n",
        "    except Exception as e:\n",
        "        print(f\"â— ê²€ìƒ‰ì°½ ì˜¤ë¥˜: {e}\")\n",
        "        continue\n",
        "\n",
        "    time.sleep(3)\n",
        "    found_matching_address = False\n",
        "\n",
        "    try:\n",
        "        search_restaurant_list = driver.find_elements(By.XPATH, \"//div[contains(@class, '_item_info_wrap_')]\")\n",
        "        for a in search_restaurant_list:\n",
        "            try:\n",
        "                add_el = a.find_element(By.XPATH, \".//button[contains(@class,'_item_address_')]\")\n",
        "                address_extracted = add_el.text.strip()\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            if address not in address_extracted:\n",
        "                continue\n",
        "\n",
        "            found_matching_address = True\n",
        "\n",
        "            try:\n",
        "                a_href_el = a.find_element(By.XPATH, \".//a[contains(@href,'/place/') and contains(@href,'/home')]\")\n",
        "                a_href = a_href_el.get_attribute(\"href\")\n",
        "            except:\n",
        "                a_href = \"\"\n",
        "\n",
        "            try:\n",
        "                name_el = a.find_element(By.XPATH, \".//strong[contains(@class,'_item_name_')]\")\n",
        "                store_name = name_el.text.strip()\n",
        "            except:\n",
        "                store_name = \"\"\n",
        "\n",
        "            try:\n",
        "                cat_el = a.find_element(By.XPATH, \".//em[contains(@class,'_item_category_')]\")\n",
        "                category = cat_el.text.strip()\n",
        "            except:\n",
        "                category = \"\"\n",
        "\n",
        "            a_href_el.click()\n",
        "            break\n",
        "\n",
        "        if not found_matching_address:\n",
        "            fail=fail+1\n",
        "            # print(f\"â— {restaurant} - ì£¼ì†Œ ë§¤ì¹­ ì‹¤íŒ¨, ê±´ë„ˆëœ€\")\n",
        "            continue\n",
        "        success = success+1\n",
        "        time.sleep(3)\n",
        "        # print(store_name)\n",
        "\n",
        "        try:\n",
        "            # XPathë¡œ ì¡°ê±´ì— ë§ëŠ” a íƒœê·¸ ì„ íƒ\n",
        "            button = driver.find_element(By.XPATH, '//a[contains(@class, \"gKP9i RMgN0\") and @aria-expanded=\"false\"]')\n",
        "            button.click()\n",
        "            time.sleep(2)\n",
        "        except Exception as e:\n",
        "            print(f\"â— ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {e}\")\n",
        "            pass\n",
        "\n",
        "        try:\n",
        "            # rows = driver.find_elements(By.XPATH, '//div[contains(@class, \"w9QyJ\")]')\n",
        "            rows = driver.find_elements(By.XPATH, '//*[@class=\"w9QyJ\"]')\n",
        "            hours_list = []\n",
        "\n",
        "            for row in rows:\n",
        "                # ìš”ì¼ ì¶”ì¶œ (ì—†ìœ¼ë©´ \"ë§¤ì¼\")\n",
        "                try:\n",
        "                    day = row.find_element(By.XPATH, './/span[contains(@class, \"i8cJw\")]').text.strip()\n",
        "                except:\n",
        "                    day = \"ë§¤ì¼\"\n",
        "\n",
        "                # ì‹œê°„ ì¶”ì¶œ (ì—†ìœ¼ë©´ \"ì •ë³´ ì—†ìŒ\")\n",
        "                time_range_elements = row.find_elements(By.XPATH, './/div[contains(@class, \"H3ua4\")]')\n",
        "                time_range = time_range_elements[0].text.strip() if time_range_elements else \"ì •ë³´ ì—†ìŒ\"\n",
        "\n",
        "                hours_list.append(f\"{day} {time_range}\")\n",
        "\n",
        "            time_combined = \" | \".join(hours_list)\n",
        "            # print(\"ğŸ•’ ì˜ì—…ì‹œê°„:\", time_combined)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"â— ì˜ì—… ì‹œê°„ ì „ì²´ ì¶”ì¶œ ì‹¤íŒ¨: {e}\")\n",
        "            time_combined = \"\"\n",
        "            pass\n",
        "        # ì •ë³´íƒ­ í´ë¦­\n",
        "        try:\n",
        "            information_tab = driver.find_element(By.XPATH, \"//a[contains(@href,'/information')]\")\n",
        "            information_tab.click()\n",
        "        except Exception as e:\n",
        "            print(f\"â— ì •ë³´ íƒ­ ì˜¤ë¥˜: {e}\")\n",
        "            pass\n",
        "        time.sleep(3)\n",
        "        # place_section_content í•˜ìœ„ì˜ owG4q ìš”ì†Œë“¤ ì„ íƒ\n",
        "        try:\n",
        "            elements = driver.find_elements(By.XPATH, '//ul[contains(@class, \"JU0iX\")]//div[@class=\"owG4q\"]')\n",
        "            facility_data = [el.text.strip() for el in elements if el.text.strip()]\n",
        "        except:\n",
        "            facility_data = []\n",
        "            pass\n",
        "        # print(facility_data)\n",
        "        time.sleep(3)\n",
        "\n",
        "        try:\n",
        "            review_tab = driver.find_element(By.XPATH, \"//a[contains(@href,'/review')]\")\n",
        "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", review_tab)\n",
        "            time.sleep(1)\n",
        "            driver.execute_script(\"arguments[0].click();\", review_tab)\n",
        "            time.sleep(2)\n",
        "            # print(\"âœ… ë¦¬ë·° íƒ­ í´ë¦­ ì„±ê³µ\")\n",
        "        except Exception as e:\n",
        "            print(f\"â— ë¦¬ë·° íƒ­ í´ë¦­ ì‹¤íŒ¨: {e}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "        try:\n",
        "            rating = driver.find_element(By.XPATH, \"//span[contains(@class,'PXMot') or contains(@class,'_1Y6hi')]\").text.strip()\n",
        "        except:\n",
        "            rating = \"\"\n",
        "\n",
        "        tag_data = []\n",
        "        # íƒœê·¸ ë”ë³´ê¸° ë°˜ë³µ í´ë¦­\n",
        "        while True:\n",
        "            try:\n",
        "                more_button = driver.find_element(By.XPATH, '//span[contains(@class, \"YqDZw\")]')\n",
        "                driver.execute_script(\"arguments[0].click();\", more_button)\n",
        "                time.sleep(1)\n",
        "            except:\n",
        "                break\n",
        "\n",
        "        # íƒœê·¸ ìš”ì†Œ ìˆ˜ì§‘\n",
        "        tag_elements = driver.find_elements(By.XPATH, '//span[contains(@class, \"t3JSf\")]')\n",
        "        for tag in tag_elements:\n",
        "            try:\n",
        "                tag_name = tag.text.strip()\n",
        "                tag_count_element = tag.find_element(By.XPATH, './following-sibling::span')\n",
        "                tag_count = int(re.sub(r'[^0-9]', '', tag_count_element.text.strip()))\n",
        "                tag_data.append((tag_name, tag_count))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        tag_data = sorted(tag_data, key=lambda x: x[1], reverse=True)  # ì „ì²´ íƒœê·¸ ìˆ˜ì§‘\n",
        "        # ë¦¬ë·° ìˆ˜ì§‘\n",
        "        all_reviews = []\n",
        "        review_limits = {\"ì¶”ì²œ\": 5, \"ìµœì‹ \": 5}\n",
        "\n",
        "        for source_label in [\"ì¶”ì²œ\", \"ìµœì‹ \"]:\n",
        "            if source_label == \"ìµœì‹ \":\n",
        "                try:\n",
        "                    latest_button = driver.find_element(By.XPATH, \"//a[@role='option' and @aria-selected='false']\")\n",
        "                    latest_button.click()\n",
        "                    time.sleep(5)\n",
        "                except:\n",
        "                    print(f\"â— ìµœì‹ ìˆœ ë²„íŠ¼ í´ë¦­ ì˜¤ë¥˜\")\n",
        "                    continue\n",
        "\n",
        "            reviews = driver.find_elements(By.XPATH, '//li[contains(@class,\"place_apply_pui\")]')\n",
        "            count = 0\n",
        "\n",
        "            for r in reviews[:review_limits[source_label]]:\n",
        "                try:\n",
        "                    content = r.find_element(By.XPATH, './/div[contains(@class,\"pui__vn15t2\")]').text.strip()\n",
        "                    date = r.find_element(By.XPATH, './/span[contains(@class,\"pui__gfuUIT\")]/time').text.strip()\n",
        "\n",
        "                    revisit_elements = r.find_elements(By.XPATH, './/span[contains(@class,\"pui__gfuUIT\")]')\n",
        "                    revisit = revisit_elements[1].text.strip() if len(revisit_elements) > 1 else ''\n",
        "                    if revisit:\n",
        "                        revisit = int(revisit.replace(\",\", \"\")[:-5])\n",
        "\n",
        "                    try:\n",
        "                        tag_expand_button = r.find_element(By.XPATH, './/a[contains(@class,\"pui__jhpEyP\")]')\n",
        "                        driver.execute_script(\"arguments[0].click();\", tag_expand_button)\n",
        "                        time.sleep(3)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    try:\n",
        "                        div = r.find_element(By.XPATH, './/div[contains(@class, \"pui__-0Ter1\")]')\n",
        "                        spans = div.find_elements(By.TAG_NAME, 'span')\n",
        "                        span_texts = [span.text.strip() for span in spans]\n",
        "                        visit_tag = span_texts[3].strip() if len(span_texts) >= 3 else \"íƒœê·¸ ì—†ìŒ\"\n",
        "                        mate_tag = span_texts[4].strip() if len(span_texts) >= 4 else \"íƒœê·¸ ì—†ìŒ\"\n",
        "                    except:\n",
        "                        visit_tag = \"íƒœê·¸ ì—†ìŒ\"\n",
        "                        mate_tag = \"íƒœê·¸ ì—†ìŒ\"\n",
        "\n",
        "                    all_reviews.append([\n",
        "                        restaurant, store_name, category, time_combined, rating, source_label,\n",
        "                        content, date, revisit,  visit_tag, mate_tag, already_crawled+crawl,total_review+review_cnt\n",
        "                    ])\n",
        "                    count += 1\n",
        "                    review_cnt = review_cnt+1\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        max_len = max(len(all_reviews), len(tag_data), len(facility_data))\n",
        "        # âœ… íƒœê·¸ê°€ 10ê°œ ë¯¸ë§Œì¼ ê²½ìš° ë¹ˆ ê°’ìœ¼ë¡œ ì±„ìš°ê¸°\n",
        "        tag_data += [(\"\", \"\")] * (max_len - len(tag_data))\n",
        "        facility_data += [\"\"] * (max_len - len(facility_data))\n",
        "        all_reviews += [\n",
        "            [restaurant, store_name, category, time_combined, rating, '', '', '', '', '', '', already_crawled + crawl, total_review + review_cnt]\n",
        "        ] * (max_len - len(all_reviews))\n",
        "\n",
        "        for i in range(max_len):\n",
        "            review = all_reviews[i]\n",
        "            tag_name, tag_count = tag_data[i]\n",
        "            facility = facility_data[i]\n",
        "\n",
        "            review_data.append([\n",
        "                *review[:11],      # ê¸°ë³¸ ë¦¬ë·° ì •ë³´ (0~10)\n",
        "                tag_name, tag_count,\n",
        "                facility,          # í¸ì˜ì‹œì„¤\n",
        "                review[11], review[12]  # í¬ë¡¤ë§ ìˆ˜, ë¦¬ë·° ìˆ˜\n",
        "            ])\n",
        "    except Exception as e:\n",
        "        print(f\"â— ê°€ê²Œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "driver.quit()\n",
        "\n",
        "# ëˆ„ì  ì €ì¥ ì²˜ë¦¬\n",
        "df_new = pd.DataFrame(review_data, columns=columns)\n",
        "\n",
        "if os.path.exists(csv_path):\n",
        "    df_old = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
        "    df_combined = pd.concat([df_old, df_new], ignore_index=True)\n",
        "else:\n",
        "    df_combined = df_new\n",
        "\n",
        "# ì €ì¥  ## ë³€ê²½í•´ì•¼í•  ë¶€ë¶„\n",
        "df_combined.to_csv(csv_path, index=False, encoding='utf-8-sig')\n",
        "print(f\"í¬ë¡¤ë§ ì„±ê³µ ê°œìˆ˜ : {success}, í¬ë¡¤ë§ ì‹¤íŒ¨ ê°œìˆ˜ : {fail}, ì´ ìˆ˜ì§‘í•œ ë¦¬ë·° ê°œìˆ˜ : {review_cnt}\")\n",
        "print(f\"ğŸ“ ë¦¬ë·° ë°ì´í„°ë¥¼ ëˆ„ì  ì €ì¥í–ˆìŠµë‹ˆë‹¤: {csv_path}\")"
      ],
      "metadata": {
        "id": "zBBM0vmxgLaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}